---
title: Zeroth-Order Topological Insights into Iterative Magnitude Pruning
abstract: Modern-day neural networks are famously large, yet also highly redundant
  and compressible; there exist numerous pruning strategies in the deep learning literature
  that yield over 90% sparser sub-networks of fully-trained, dense architectures while
  still maintaining their original accuracies. Amongst these many methods though –
  thanks to its conceptual simplicity, ease of implementation, and efficacy – Iterative
  Magnitude Pruning (IMP) dominates in practice and is the de facto baseline to beat
  in the pruning community. However, theoretical explanations as to why a simplistic
  method such as IMP works at all are few and limited. In this work, we leverage persistent
  homology to show that IMP inherently encourages retention of those weights which
  preserve topological information in a trained network. Subsequently, we also provide
  bounds on how much different networks can be pruned while perfectly preserving their
  zeroth order topological features, and present a modified version of IMP to do the
  same.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: balwani22a
month: 0
tex_title: Zeroth-Order Topological Insights into Iterative Magnitude Pruning
firstpage: 6
lastpage: 16
page: 6-16
order: 6
cycles: false
bibtex_author: Balwani, Aishwarya and Krzyston, Jakob
author:
- given: Aishwarya
  family: Balwani
- given: Jakob
  family: Krzyston
date: 2022-11-09
address:
container-title: Proceedings of Topological, Algebraic, and Geometric Learning Workshops
  2022
volume: '196'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 11
  - 9
pdf: https://proceedings.mlr.press/v196/balwani22a/balwani22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
