---
title: Nearest Class-Center Simplification through Intermediate Layers
abstract: Recent advances in neural network theory have introduced geometric properties
  that occur during training, past the Interpolation Threshold- where the training
  error reaches zero. We inquire into the phenomena coined \emph{Neural Collapse}
  in the intermediate layers of the network, and emphasize the innerworkings of Nearest
  Class-Center Mismatch inside a deepnet. We further show that these processes occur
  both in vision and language model architectures. Lastly, we propose a Stochastic
  Variability-Simplification Loss (SVSL) that encourages better geometrical features
  in intermediate layers, yielding improvements in both train metrics and generalization.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ben-shaul22a
month: 0
tex_title: Nearest Class-Center Simplification through Intermediate Layers
firstpage: 37
lastpage: 47
page: 37-47
order: 37
cycles: false
bibtex_author: Ben-Shaul, Ido and Dekel, Shai
author:
- given: Ido
  family: Ben-Shaul
- given: Shai
  family: Dekel
date: 2022-11-09
address:
container-title: Proceedings of Topological, Algebraic, and Geometric Learning Workshops
  2022
volume: '196'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 11
  - 9
pdf: https://proceedings.mlr.press/v196/ben-shaul22a/ben-shaul22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
